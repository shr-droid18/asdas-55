{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e87df8b-916e-4247-8e4f-62549ab492af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2531 files belonging to 4 classes.\n",
      "Found 633 files belonging to 4 classes.\n",
      "Epoch 1/3\n",
      "\u001b[1m18/50\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:57\u001b[0m 4s/step - accuracy: 0.4181 - loss: 46.5314"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Rescaling, GlobalAveragePooling2D, Multiply\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Channel Attention Module\n",
    "def channel_attention(input_feature, ratio=8):\n",
    "    channel = input_feature.shape[-1]\n",
    "    shared_layer_one = Dense(channel // ratio, activation='relu', kernel_initializer='he_normal', use_bias=True)\n",
    "    shared_layer_two = Dense(channel, kernel_initializer='he_normal', use_bias=True)\n",
    "    \n",
    "    avg_pool = GlobalAveragePooling2D()(input_feature)\n",
    "    avg_pool = layers.Reshape((1, 1, channel))(avg_pool)\n",
    "    avg_pool = shared_layer_one(avg_pool)\n",
    "    avg_pool = shared_layer_two(avg_pool)\n",
    "    \n",
    "    max_pool = layers.GlobalMaxPooling2D()(input_feature)\n",
    "    max_pool = layers.Reshape((1, 1, channel))(max_pool)\n",
    "    max_pool = shared_layer_one(max_pool)\n",
    "    max_pool = shared_layer_two(max_pool)\n",
    "    \n",
    "    cbam_feature = layers.Add()([avg_pool, max_pool])\n",
    "    cbam_feature = layers.Activation('sigmoid')(cbam_feature)\n",
    "    \n",
    "    return Multiply()([input_feature, cbam_feature])\n",
    "\n",
    "# Objective function for optimization\n",
    "def objective_function(params, train_dataset_sample, validation_dataset_sample):\n",
    "    dropout_rate, learning_rate = params\n",
    "    \n",
    "    # Ensure dropout_rate is in the range [0, 1)\n",
    "    dropout_rate = np.clip(dropout_rate, 0.0, 0.5)\n",
    "    \n",
    "    model = create_model(dropout_rate, learning_rate)\n",
    "    \n",
    "    # Train using subset to save time\n",
    "    history = model.fit(train_dataset_sample, epochs=3, validation_data=validation_dataset_sample)\n",
    "    \n",
    "    # Validation loss as metric\n",
    "    val_loss = min(history.history['val_loss'])\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "# Firefly algorithm for hyperparameter tuning\n",
    "def firefly_algorithm(n, max_gen, train_dataset_sample, validation_dataset_sample, alpha=0.5, beta_min=0.2, gamma=1.0):\n",
    "    fireflies = np.random.uniform(low=[0.0, 1e-5], high=[0.5, 1e-3], size=(n, 2))\n",
    "    fitness = np.zeros(n)\n",
    "    \n",
    "    for gen in range(max_gen):\n",
    "        for i in range(n):\n",
    "            fitness[i] = objective_function(fireflies[i], train_dataset_sample, validation_dataset_sample)\n",
    "            \n",
    "        # Sort fireflies based on fitness\n",
    "        indices = np.argsort(fitness)\n",
    "        fireflies = fireflies[indices]\n",
    "        \n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if fitness[i] > fitness[j]:\n",
    "                    r = np.linalg.norm(fireflies[i] - fireflies[j])\n",
    "                    beta = beta_min + (1 - beta_min) * np.exp(-gamma * r ** 2)\n",
    "                    fireflies[i] += beta * (fireflies[j] - fireflies[i]) + alpha * (np.random.rand(2) - 0.5)\n",
    "                    fireflies[i] = np.clip(fireflies[i], [0.0, 1e-5], [0.5, 1e-3])\n",
    "                    \n",
    "    best_firefly = fireflies[0]\n",
    "    return best_firefly\n",
    "\n",
    "# CNN Model with attention and dropout\n",
    "def create_model(dropout_rate, learning_rate):\n",
    "    inputs = layers.Input(shape=(224, 224, 3))\n",
    "    x = Rescaling(1./255)(inputs)\n",
    "    \n",
    "    # Data Augmentation\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "        layers.RandomRotation(0.2),\n",
    "        layers.RandomZoom(0.2),\n",
    "    ])\n",
    "    x = data_augmentation(x)\n",
    "    \n",
    "    # Convolution Block 1\n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3), kernel_regularizer=l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = channel_attention(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Additional Convolution Blocks (Reduced filters for faster training)\n",
    "    for filters in [128, 256]:\n",
    "        x = Conv2D(filters=filters, kernel_size=(3, 3), kernel_regularizer=l2(0.001))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = channel_attention(x)\n",
    "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Fully Connected Layers\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(units=256, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(units=4, activation='softmax')(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Load and sample the dataset\n",
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    'archive/split_leaves/train',\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    label_mode='categorical'\n",
    ")\n",
    "validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    'archive/split_leaves/validation',\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    label_mode='categorical'\n",
    ")\n",
    "\n",
    "# Subset for optimization\n",
    "train_dataset_sample = train_dataset.take(50)  # Small subset for tuning\n",
    "validation_dataset_sample = validation_dataset.take(20)\n",
    "\n",
    "# Firefly Algorithm Optimization\n",
    "best_params = firefly_algorithm(n=5, max_gen=10, train_dataset_sample=train_dataset_sample, validation_dataset_sample=validation_dataset_sample)\n",
    "best_dropout_rate, best_learning_rate = best_params\n",
    "\n",
    "# Final model training with optimized hyperparameters and Early Stopping\n",
    "model = create_model(best_dropout_rate, best_learning_rate)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "history = model.fit(train_dataset, epochs=30, validation_data=validation_dataset, callbacks=[early_stopping])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76ee5db9-a4a1-4eab-8abd-5d3c394e6e02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b60a6a-d4b9-4ffd-86de-981c33bde6fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
